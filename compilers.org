#+TITLE: David's Notes on Compilers
#+AUTHOR: David Udelson
#+DESCRIPTION: Notes on how compilers work and how to build one
#+FILETAGS:
#+LANGUAGE: en
#+OPTIONS: toc:nil h:4 html-postamble:nil html-preamble:t tex:t f:t
#+OPTIONS: prop:("VERSION")
#+HTML_DOCTYPE: <!DOCTYPE html>
#+HTML_HEAD: <link href="http://fonts.googleapis.com/css?family=Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css" />
#+HTML_HEAD: <link href="/home/david/s/doc/org-html-export.css" rel="stylesheet" type="text/css" />

#+TOC: headlines 3

#+BEGIN_COMMENT
This org document is configured for export to nicely-formatted html. The html
template can be found on github at https://github.com/thi-ng/org-spec/. There is
also an example file available at http://demo.thi.ng/org-spec/ which
demonstrates how various org-mode elements look when exported. If you are trying
to figure out how to write the markup for a certain element (e.g. a definition
list, an image caption, etc.), it is suggested to find that element in the
example doc and then refer to the corresponding portion of the example file raw
markup (on github) in order to learn the correct syntax. Note that the template
is intended for technical documents; however despite this I find it works quite
nicely for my notes.
#+END_COMMENT
https://lagunita.stanford.edu/courses/Engineering/Compilers/Fall2014/info
http://web.stanford.edu/class/archive/cs/cs143/cs143.1128/
https://www.di.ens.fr/~cousot/AI/IntroAbsInt.html
thunk: supposed to be the facetious past participle of "think", because a thunk
"saves a computation that was already thunk up".
* About These Notes
These notes were taken for Cornell's CS 4120: Introduction to Compilers during
the Spring 2018 semester.
* U0: Introduction
:PROPERTIES:
:ID:       81f33e7d-b1f8-4cbb-9ca1-b3b49485c2f2
:Attachments: screenshot_2018-02-26_17-33-30.png screenshot_2018-02-26_17-34-18.png screenshot_2018-02-26_17-35-31.png
:END:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-02-26 17:33:30
#+CAPTION: The big picture
[[file:media/81/f33e7d-b1f8-4cbb-9ca1-b3b49485c2f2/screenshot_2018-02-26_17-33-30.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2018-02-26 17:34:18
#+CAPTION: Compilation broken down into passes, pt. 1
[[file:media/81/f33e7d-b1f8-4cbb-9ca1-b3b49485c2f2/screenshot_2018-02-26_17-34-18.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2018-02-26 17:35:32
#+CAPTION: Compilation broken down into steps, pt. 2
[[file:media/81/f33e7d-b1f8-4cbb-9ca1-b3b49485c2f2/screenshot_2018-02-26_17-35-31.png]]

At every stage of the compiler, we use theoretical CS / mathematics to formalize
the transformation we want the compiler to perform on the input program. For
example, lexical analysis is formalized via regular expressions, syntactic
analysis is formalized via context-free grammars (written in BNF), semantic
analysis via type inference rules, etc.
* U1: Lexing and Parsing
** Lexing (aka Lexical Analysis)
Summary:
    - A _lexical analyzer_ ("_lexer_") converts a *text stream to tokens*
    - This is actually a non-trivial problem, and ad-hoc lexers are hard to
      implement correctly and maintain
    - For most languages, legal tokens are conveniently and precisely defined
      using regular expressions (REs).
    - _Lexer generators_ generate lexer code automatically from token REs
*** Lexing Overview
A lexer converts a text stream (source code) into a sequence of _tokens_.
Whitespace and tokens are typically thrown away during the lexing process.
Tokens can have one or more attributes, such as the token's value or location in
the source stream (file name, line number, column number).
**** TODO insert picture: lecture 2, slide 6
**** TODO insert picture: lecture 2, slide 7
*** Approaches to Lexing
**** Ad-Hoc Lexing
_Ad-Hoc Lexing_ is just rolling your own code from scratch to lex your program
into tokens. This is the "naive" approach, because lexing is actually a
non-trivial task even for very simple languages. Some problems:
    1. Since the source stream is read one character at a time, we would prefer
       our lexer to have a /lookahead/ property, where we can "peek" at the next
       character in the text stream without consuming it.
    2. The programmer may be tempted to optimize the lexer for performance,
       which is often in tension with security concerns.
**** Lexing using Regular Expressions
Because ad-hoc lexing is difficult to get right, usually lexers use regular
expressions (REs) to describe what tokens they're looking for in the source
stream (note that a regular expression engine implements the lookahead
property).

Example: "(-?[1-9][0-9]*)|0" is an RE that describes integer literals.

***** TODO insert pictures with details about regular expressions
lecture 2 slides 13, 14, 15, 16
*** Lexer Generators
Lexer generators are programs that take in a description of what the tokens to
lex look like and generate lexer code automatically. The input is typically a
file in a custom format which describes each token type in terms of regular
expressions, as well as what /actions/ to perform when a token of that type is
identified. Examples of actions:
    - create and return token (common case)
    - discard (e.g. for comments)
    - update lexer state (advanced case, e.g. for pretty-printers)

The last example eludes to the fact that many modern lexer generators allow for
the creation of internal state that is updated based on certain features of the
source stream. For example, a pretty-printer needs to parse a source file in
order to manipulate it, but may also need to retain comments for
pretty-printing. Therefore the lexer generator may generate a lexer that records
the position of each comment in the source stream.

Lexer generators often also allow for the creation of /aliases/, which make it
possible to specify tokens in a more composable way. But these aliases cannot be
used recursively, because that's beyond the power of regular expressions (it's a
context-free grammar instead).

If we have something like "elsex = 0", we do not want to accidentally lex this
into the tokens "else", "x", "=", and "0". This illustrates that when lexing,
*the longest token wins*.
** Parsing (aka Syntactic Analysis)
:PROPERTIES:
:ID:       947c7947-46a2-4d97-8f73-83874e540efc
:Attachments: screenshot_2018-02-09_20-05-30.png screenshot_2018-02-09_20-09-02.png screenshot_2018-02-09_20-10-09.png
:END:
_top-down parsing_: Recognize the "highest-level" components of the AST first,
then derive the lower elements.

_bottom-up parsing_: Recognize the "lowest-level" components of the AST first,
then derive the higher elements.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-02-09 20:05:31
#+CAPTION: Example parse tree for "A = B + C*2; D = 1".
#+CAPTION: From wikipedia.
[[file:media/94/7c7947-46a2-4d97-8f73-83874e540efc/screenshot_2018-02-09_20-05-30.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2018-02-09 20:09:03
#+CAPTION: Bottom-up parse steps for the above parse tree.
#+CAPTION: From wikipedia.
[[file:media/94/7c7947-46a2-4d97-8f73-83874e540efc/screenshot_2018-02-09_20-09-02.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2018-02-09 20:10:09
#+CAPTION: Top-down parse steps for the above parse tree.
#+CAPTION: From wikipedia.
[[file:media/94/7c7947-46a2-4d97-8f73-83874e540efc/screenshot_2018-02-09_20-10-09.png]]

*NOTE* that the terms "top-down" and "bottom-up" do /not/ refer to the order in
 which the input file is read, but rather the way in which the AST is built up.
 Both methods generally read the input left to right and top to bottom. However,
 the first "L" in "LL" and "LR" /does/ stand for "left to right", in reference
 to the order the input is scanned.
 
_LL parsers_: Class of parsers which read input from *l* eft to right and
produce a *l* eftmost derivation. All LL parsers are top-down parsers.

_LR parsers_: Class of parsers which read input from *l* eft to right and
produce a *r* ightmost derivation. All LR parsers are bottom-up parsers.

In practice LR parsers (in particular LALR parsers) are used, as they have
several advantages over LL parsers, as we will see.

From [[https://en.wikipedia.org/wiki/LR_parser][wikipedia]]:
LR parsers can handle a larger range of languages and grammars than precedence
parsers or top-down LL parsing.[3] This is because the LR parser waits until it
has seen an entire instance of some grammar pattern before committing to what it
has found. An LL parser has to decide or guess what it is seeing much sooner,
when it has only seen the leftmost input symbol of that pattern. LR is also
better at error reporting. It detects syntax errors as early in the input stream
as possible.

===============================================================================

REs are not powerful enough to describe language grammars. This is because we
learned from lexing that an RE corresponds to a DFA, which by definition has a
finite number of states. However, in order to describe language grammars, we
need /unbounded counting/, which is something no finite automaton can do. For an
example that illustrates why we need unbounded counting, consider the simple
language of well-formed pairs of nested parentheses. In order to know if we
have enough close parens, we need to know how many open parens we saw, and this
number is unbounded. For this reason, we use the more powerful _context-free
grammar_.

Derivation => Parse Tree =(shed unneeded info from parse tree)=> AST

An ambiguous grammar allows multiple valid parse trees for the same expression,
which means the meaning of the expression is not well-defined.

Typically ambiguity can be eliminated by only allowing recursion on one side of
a production (left recursion => left associativity, ditto for right).

===============================================================================

Requirements for a grammar to be LL(k):
    - no ambiguity in the grammar
    - no left-recursion

Another way of stating the same requirements is that a predictive parsing table
(PPT) with k lookahead symbols can always be built for an LL(k) grammar. So if a
grammar is not LL(k) for any k, then no PPT can be constructed.

I.e. LL(k) grammar
    =(yields)=> predictive parsing table (maps lookahead char(s) to next production)
    =(yields)=> recursive descent parser (set of mutually recursive functions, one for
    each production, each of which switches (or matches) on the lookahead char(s))
    
Thus in order to implement a top-down parser for an LL(1) grammar, we need an
algorithm for constructing a PPT from the grammar. (Going from the PPT to the
implementation code is trivial.) In order to develop this algorithm, we need
some information, which we will define as follows:
    - FIRST(A): the set of terminals that could begin the fully-expanded version of
      production A
    - FOLLOW(A): the set of terminals that could immediately follow the production A
    - NULLABLE(A): whether the production A could derive the empty string
      
Then a cell (X, a) of the PPT (where X is a non-terminal and a is the lookahead
char) contains a production X -> g if:
    1. a is in FIRST(g); or
    2. NULLABLE(g) and a is in FOLLOW(X)

By these rules, each of the cells of the PPT must end up in one of the following
three states:
    1. The cell is empty. This means that seeing the lookahead char a when the
       non-terminal to be expanded is X is a syntax error.
    2. The cell has one production. This means that seeing the lookahead char a
       when the non-terminal to be expanded is X will result in X being expanded
       according to the RHS of the production in the cell.
    3. The cell has more than one production. This means that the grammar is not
       LL(1).
       
===============================================================================

Compare and contrast the top-down approach with the bottom-up approach:
    - top-down: As soon as I see the next lookahead symbol, I need to be able to
      know ("predict") which production that symbol is a part of.
    - bottom-up: Put each symbol I read on a stack, and transform them into
      productions once I have enough information to know what I'm looking at.

The bottom-up approach is more flexible and powerful, as you can see (since we
don't need to left-factor everything).

The problem with the shift-reduce approach is sometimes we /can/ reduce but
/shouldn't/. For example, the reduction "X -> empty string" can always be
applied, but clearly this is usually not what we want. Also, sometimes we can
apply more than one valid reduction, and we want to know which one is correct.
